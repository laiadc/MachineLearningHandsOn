{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsaYUqFDl5y1"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/laiadc/MachineLearningHandsOn/blob/main/day3/Ethics%20and%20sentiment%20analysis.ipynb)\n",
    "\n",
    "# Ethics in Sentiment Analysis\n",
    "\n",
    "A very popular field in machine learning is natural language processing (NLP). This field is devoted to extract valuable information from text. Examples of applications of NLP are data translation ([google translaor](https://translate.google.es/?hl=ca) is an example, even though you can do much better), gramatical correction, summarizing text, etc. All these applications have something in common: they work with words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2c3Wah5_u7B"
   },
   "source": [
    "## Woking with words\n",
    "\n",
    "In order to use a machine learning method to work with words and text, we need to give the words to the model in a way it can understand them. Machine learning models can not work with words directly, they need the words to be represented as a vector of fixed length.\n",
    "\n",
    "For example, the word \"apple\" could be represented as a vector or *embedding*:\n",
    "\n",
    "$$\n",
    "embedding(\\text{apple}) = \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "In general, if we have a set $V$ of words, we would represent the words as a matrix (or simply a table):\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "word_1: \\\\\n",
    "word_2:\\\\\n",
    "\\vdots\\\\\n",
    "word_{|V|}: \\\\\n",
    "\\end{matrix}\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x_{1,1}&x_{1,2}& \\dots &x_{1,n}\\\\\n",
    "x_{2,1}&x_{2,2}& \\dots &x_{2,n}\\\\\n",
    "\\vdots&&\\\\\n",
    "x_{{|V|},1}&x_{{|V|},2}& \\dots &x_{{|V|},n}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The job of data scientists is to find the representation of words as vectors which is more useful for the machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpDk5GfMEkmV"
   },
   "source": [
    "## One-hot encoding\n",
    "\n",
    "The simplest way to associate words with vectors is to use the one-hot encoding technique. Ee represent every word as an vector with all $0$s and one $1$ at the index of that word in the sorted english language. Word vectors in this type of encoding would appear as the following:\n",
    "\n",
    "<centering>\n",
    "$$w^{aardvark} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{a} = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right] , w^{at} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{array} \\right] , \\cdots,  w^{zebra} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{array} \\right] $$\n",
    "</centering>\n",
    "\n",
    "\n",
    "We represent each word as a completely independent entity:\n",
    "\n",
    "$$(w^{hotel})^Tw^{motel} = (w^{hotel})^Tw^{cat} = 0$$\n",
    "\n",
    "What is the problem with this approach? \n",
    "1. There are around ~ 500.000 words in English (~100.000 in Spanish), and therefore every vector is way too large.\n",
    "\n",
    "2. Not all the words are equally common in texts (words like \"él\", \"la\", \"uno\" are much more frequent than \"electrocardiograma\" in Spanish, for instance). Then, should all the words have the same importance? \n",
    "\n",
    "3. We are not taking into account gramatical and semantical rules. Words are not independent from each other. For example, the sentence \"*María fue a comprar patatas*\" is much more frequent than \"*Comer silla es juguete*\", which has no meaning. \n",
    "\n",
    "Luckily, deep learning models are able to take all these issues into account and design more useful word embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzlvKsdWl5y6"
   },
   "source": [
    "## Word embeddings\n",
    "\n",
    "Modern word embeddings assume that words that are related will often appear in the same documents (or sentences, paragraphs, etc.).\n",
    "\n",
    "For instance, \"banco\", \"dinero\", \"inversiones\", \"deudad\", etc. are probably likely to appear together. But \"banco\", \"pulpo\", \"plátano\", and \"hockey\" would probably not consistently appear together. These models use complex algorithms which allow to represent each word as a vector which encodes information about the probability of appearing next to other words, the popularity of this word and the gramatical and semantical rules of the language. As you can imagine, this is not an easy task. \n",
    "\n",
    "\n",
    "There are several datasets of pre-trained English word embeddings such as `word2vec`, pretrained on Google News data, and `GloVe`, pretrained on the Common Crawl of web pages. We will use `GloVe`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f-cnKmOJ-I7"
   },
   "source": [
    "## Load the embeddings\n",
    "\n",
    "The following code downloads the data to create the embeddings from words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T09:08:49.211876Z",
     "start_time": "2019-03-29T09:01:34.100348Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoP3dOkrl5y8",
    "outputId": "ae667a4c-7db3-495a-e175-4a81bb2d2985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-06 14:50:34--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
      "--2021-05-06 14:50:34--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
      "--2021-05-06 14:50:34--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1877800501 (1.7G) [application/zip]\n",
      "Saving to: ‘glove.42B.300d.zip’\n",
      "\n",
      "glove.42B.300d.zip  100%[===================>]   1.75G  5.00MB/s    in 5m 55s  \n",
      "\n",
      "2021-05-06 14:56:30 (5.04 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T09:10:02.327170Z",
     "start_time": "2019-03-29T09:09:24.646719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngmpNJi7l5zC",
    "outputId": "04930b91-bdc8-473c-9df2-9dc38bdc8f93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"unzip\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!unzip glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:04:47.628009Z",
     "start_time": "2019-03-29T15:01:58.108263Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ccEbFw1l5zF",
    "outputId": "a4ff96a4-e27c-4c5d-e490-b137dbe4fcbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [02:49, 11326.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1917494, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in tqdm(enumerate(infile)):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "embeddings = load_embeddings('glove.42B.300d.txt')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD93OLQAKRrb"
   },
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "In this example we will use the word embeddings to perform what is called sentiment analysis. This technique consists of assigning a value to each word. Positive values refer to positive words (such as \"happy\", \"good\", \"awesome\", etc.) and negative values refer to bad words (such as \"hard\", \"sad\", \"boring\", etc.) Values close to zero refer to neutral words. This method can be used to automatically asses product reviews just by reading the text. \n",
    "\n",
    "The next cells show the steps to create a simple model to perform sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gclCP_82l5zI"
   },
   "source": [
    "## Positive and Negative Words\n",
    "\n",
    "We need some input about which words are positive and which words are negative. There are many sentiment lexicons you could use, but we’re going to go with a very straightforward lexicon from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html  \n",
    "\n",
    "There is a copy of these files in the GitHub repository of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgpHh__o8FZr",
    "outputId": "c141b941-1671-4024-8260-a871c0314c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-06 15:03:54--  https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster20192020/master/data/positive-words.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22408 (22K) [text/plain]\n",
      "Saving to: ‘positive-words.txt’\n",
      "\n",
      "\r",
      "positive-words.txt    0%[                    ]       0  --.-KB/s               \r",
      "positive-words.txt  100%[===================>]  21.88K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-05-06 15:03:54 (33.4 MB/s) - ‘positive-words.txt’ saved [22408/22408]\n",
      "\n",
      "--2021-05-06 15:03:54--  https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster20192020/master/data/negative-words.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50857 (50K) [text/plain]\n",
      "Saving to: ‘negative-words.txt’\n",
      "\n",
      "negative-words.txt  100%[===================>]  49.67K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-05-06 15:03:54 (57.9 MB/s) - ‘negative-words.txt’ saved [50857/50857]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster20192020/master/data/positive-words.txt'\n",
    "\n",
    "!wget 'https://raw.githubusercontent.com/DataScienceUB/DeepLearningMaster20192020/master/data/negative-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:04:56.665741Z",
     "start_time": "2019-03-29T15:04:56.619197Z"
    },
    "id": "4Yt4FnCdl5zI"
   },
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('positive-words.txt')\n",
    "neg_words = load_lexicon('negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bIIPorXl5zL"
   },
   "source": [
    "Some of these words are not in the GloVe vocabulary. We have to remove them so that we do not have errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:04:59.940580Z",
     "start_time": "2019-03-29T15:04:58.168200Z"
    },
    "id": "yM9TIX0al5zM"
   },
   "outputs": [],
   "source": [
    "pos_vectors = embeddings.loc[embeddings.index.intersection(pos_words)]\n",
    "neg_vectors = embeddings.loc[embeddings.index.intersection(neg_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gDJtO6-Ncum"
   },
   "source": [
    "Here we can see an example of positive and negative words, together with their respective embeddings. Notice that each word is represented as a vector (the row of the table) of 300 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Ox2j1aZ_NSY2",
    "outputId": "996c5f26-a7ef-4b3e-c8d1-56f00ac0d466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE WORDS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.014949</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>-0.288830</td>\n",
       "      <td>-0.33999</td>\n",
       "      <td>-0.030165</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>-4.0142</td>\n",
       "      <td>-0.561890</td>\n",
       "      <td>-0.245360</td>\n",
       "      <td>-0.90196</td>\n",
       "      <td>0.426050</td>\n",
       "      <td>0.30041</td>\n",
       "      <td>0.32461</td>\n",
       "      <td>-0.094574</td>\n",
       "      <td>-0.133660</td>\n",
       "      <td>-0.049502</td>\n",
       "      <td>-0.249840</td>\n",
       "      <td>0.023725</td>\n",
       "      <td>-0.397180</td>\n",
       "      <td>0.172910</td>\n",
       "      <td>0.062534</td>\n",
       "      <td>-0.34646</td>\n",
       "      <td>0.296720</td>\n",
       "      <td>-0.119390</td>\n",
       "      <td>0.13938</td>\n",
       "      <td>-0.505840</td>\n",
       "      <td>-0.215180</td>\n",
       "      <td>-0.730680</td>\n",
       "      <td>-0.051667</td>\n",
       "      <td>-0.069903</td>\n",
       "      <td>-0.071621</td>\n",
       "      <td>0.389340</td>\n",
       "      <td>0.170070</td>\n",
       "      <td>0.190840</td>\n",
       "      <td>0.040775</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.327190</td>\n",
       "      <td>-0.311380</td>\n",
       "      <td>0.34084</td>\n",
       "      <td>-0.180780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27485</td>\n",
       "      <td>-0.234080</td>\n",
       "      <td>0.334140</td>\n",
       "      <td>-0.336470</td>\n",
       "      <td>-0.099405</td>\n",
       "      <td>-0.112080</td>\n",
       "      <td>-0.272000</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>-0.188090</td>\n",
       "      <td>0.025763</td>\n",
       "      <td>0.038478</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>-0.42713</td>\n",
       "      <td>-0.216490</td>\n",
       "      <td>0.030883</td>\n",
       "      <td>-0.232090</td>\n",
       "      <td>0.186090</td>\n",
       "      <td>0.342090</td>\n",
       "      <td>0.097109</td>\n",
       "      <td>-0.166160</td>\n",
       "      <td>1.135300</td>\n",
       "      <td>0.186620</td>\n",
       "      <td>0.22876</td>\n",
       "      <td>0.081640</td>\n",
       "      <td>0.089013</td>\n",
       "      <td>-0.14447</td>\n",
       "      <td>0.064785</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.250460</td>\n",
       "      <td>-0.06917</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>-0.040679</td>\n",
       "      <td>-0.346990</td>\n",
       "      <td>-0.315360</td>\n",
       "      <td>0.47710</td>\n",
       "      <td>-0.234870</td>\n",
       "      <td>-0.33792</td>\n",
       "      <td>0.059178</td>\n",
       "      <td>-0.192210</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>-0.069254</td>\n",
       "      <td>0.37668</td>\n",
       "      <td>-0.169580</td>\n",
       "      <td>-0.27482</td>\n",
       "      <td>0.256670</td>\n",
       "      <td>-0.202930</td>\n",
       "      <td>-4.1122</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>-0.270850</td>\n",
       "      <td>-0.87003</td>\n",
       "      <td>-0.010262</td>\n",
       "      <td>-0.10048</td>\n",
       "      <td>-0.06171</td>\n",
       "      <td>-0.211100</td>\n",
       "      <td>-0.376520</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.054106</td>\n",
       "      <td>-0.252250</td>\n",
       "      <td>-0.125050</td>\n",
       "      <td>-0.134610</td>\n",
       "      <td>0.340040</td>\n",
       "      <td>-0.28599</td>\n",
       "      <td>0.048589</td>\n",
       "      <td>-0.088778</td>\n",
       "      <td>-0.10750</td>\n",
       "      <td>-0.411750</td>\n",
       "      <td>-0.153960</td>\n",
       "      <td>0.043628</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.072977</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>0.178690</td>\n",
       "      <td>0.033359</td>\n",
       "      <td>-0.147760</td>\n",
       "      <td>0.296360</td>\n",
       "      <td>-0.022996</td>\n",
       "      <td>-0.147270</td>\n",
       "      <td>-0.131910</td>\n",
       "      <td>-0.10101</td>\n",
       "      <td>-0.087417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15689</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.299280</td>\n",
       "      <td>-0.133760</td>\n",
       "      <td>0.512290</td>\n",
       "      <td>-0.058114</td>\n",
       "      <td>0.051375</td>\n",
       "      <td>-0.379820</td>\n",
       "      <td>-0.099555</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>-0.196080</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>-0.20517</td>\n",
       "      <td>-0.041098</td>\n",
       "      <td>-0.273480</td>\n",
       "      <td>0.059709</td>\n",
       "      <td>-0.261000</td>\n",
       "      <td>0.054030</td>\n",
       "      <td>-0.367590</td>\n",
       "      <td>0.127250</td>\n",
       "      <td>0.678610</td>\n",
       "      <td>-0.463270</td>\n",
       "      <td>0.17716</td>\n",
       "      <td>0.342190</td>\n",
       "      <td>0.490060</td>\n",
       "      <td>0.47282</td>\n",
       "      <td>-0.039460</td>\n",
       "      <td>0.393040</td>\n",
       "      <td>-0.023976</td>\n",
       "      <td>-0.60973</td>\n",
       "      <td>-0.484760</td>\n",
       "      <td>-0.333630</td>\n",
       "      <td>-0.075956</td>\n",
       "      <td>0.158270</td>\n",
       "      <td>0.29431</td>\n",
       "      <td>-0.074275</td>\n",
       "      <td>-0.28909</td>\n",
       "      <td>-0.220280</td>\n",
       "      <td>0.088971</td>\n",
       "      <td>-0.028352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>0.088205</td>\n",
       "      <td>0.40155</td>\n",
       "      <td>-0.044703</td>\n",
       "      <td>-0.50973</td>\n",
       "      <td>0.116040</td>\n",
       "      <td>-0.161300</td>\n",
       "      <td>-4.3066</td>\n",
       "      <td>-0.084823</td>\n",
       "      <td>0.088231</td>\n",
       "      <td>-0.59067</td>\n",
       "      <td>0.050804</td>\n",
       "      <td>0.15545</td>\n",
       "      <td>0.20166</td>\n",
       "      <td>0.125620</td>\n",
       "      <td>-0.243730</td>\n",
       "      <td>-0.338920</td>\n",
       "      <td>-0.103340</td>\n",
       "      <td>-0.231480</td>\n",
       "      <td>-0.076147</td>\n",
       "      <td>0.166520</td>\n",
       "      <td>0.042690</td>\n",
       "      <td>0.19294</td>\n",
       "      <td>0.159880</td>\n",
       "      <td>-0.055858</td>\n",
       "      <td>-0.13619</td>\n",
       "      <td>-0.304690</td>\n",
       "      <td>0.258080</td>\n",
       "      <td>-0.337290</td>\n",
       "      <td>0.374110</td>\n",
       "      <td>0.215360</td>\n",
       "      <td>-0.122770</td>\n",
       "      <td>-0.045159</td>\n",
       "      <td>0.127970</td>\n",
       "      <td>-0.035766</td>\n",
       "      <td>-0.004277</td>\n",
       "      <td>-0.490100</td>\n",
       "      <td>-0.070168</td>\n",
       "      <td>-0.030209</td>\n",
       "      <td>0.16469</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12287</td>\n",
       "      <td>-0.040766</td>\n",
       "      <td>-0.038844</td>\n",
       "      <td>-0.412460</td>\n",
       "      <td>0.298020</td>\n",
       "      <td>-0.149480</td>\n",
       "      <td>-0.148860</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.270100</td>\n",
       "      <td>-0.128980</td>\n",
       "      <td>-0.007672</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>-0.13574</td>\n",
       "      <td>0.018512</td>\n",
       "      <td>-0.067022</td>\n",
       "      <td>0.256120</td>\n",
       "      <td>-0.321570</td>\n",
       "      <td>0.069910</td>\n",
       "      <td>-0.165410</td>\n",
       "      <td>-0.058677</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>-0.440770</td>\n",
       "      <td>0.22677</td>\n",
       "      <td>0.179880</td>\n",
       "      <td>0.251690</td>\n",
       "      <td>0.45095</td>\n",
       "      <td>-0.105110</td>\n",
       "      <td>0.191540</td>\n",
       "      <td>0.038259</td>\n",
       "      <td>-0.21491</td>\n",
       "      <td>0.042501</td>\n",
       "      <td>-0.043897</td>\n",
       "      <td>-0.550510</td>\n",
       "      <td>-0.274440</td>\n",
       "      <td>0.31041</td>\n",
       "      <td>-0.230230</td>\n",
       "      <td>-0.24066</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>0.079135</td>\n",
       "      <td>0.095058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-0.033819</td>\n",
       "      <td>0.27263</td>\n",
       "      <td>-0.203010</td>\n",
       "      <td>-0.29310</td>\n",
       "      <td>0.104890</td>\n",
       "      <td>-0.211420</td>\n",
       "      <td>-3.5503</td>\n",
       "      <td>-0.633350</td>\n",
       "      <td>-0.282800</td>\n",
       "      <td>-0.45047</td>\n",
       "      <td>-0.366120</td>\n",
       "      <td>-0.15207</td>\n",
       "      <td>0.20128</td>\n",
       "      <td>0.186320</td>\n",
       "      <td>-0.073614</td>\n",
       "      <td>-0.083101</td>\n",
       "      <td>0.079977</td>\n",
       "      <td>-0.172940</td>\n",
       "      <td>0.063676</td>\n",
       "      <td>0.141480</td>\n",
       "      <td>0.495160</td>\n",
       "      <td>0.19244</td>\n",
       "      <td>0.056514</td>\n",
       "      <td>0.077089</td>\n",
       "      <td>0.11047</td>\n",
       "      <td>-0.233730</td>\n",
       "      <td>0.251210</td>\n",
       "      <td>0.360610</td>\n",
       "      <td>-0.048826</td>\n",
       "      <td>0.187960</td>\n",
       "      <td>-0.090890</td>\n",
       "      <td>0.133780</td>\n",
       "      <td>0.354210</td>\n",
       "      <td>0.330030</td>\n",
       "      <td>0.116130</td>\n",
       "      <td>-0.364060</td>\n",
       "      <td>-0.053876</td>\n",
       "      <td>-0.244510</td>\n",
       "      <td>0.55024</td>\n",
       "      <td>-0.230970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31822</td>\n",
       "      <td>0.234780</td>\n",
       "      <td>0.146620</td>\n",
       "      <td>-0.059459</td>\n",
       "      <td>0.344960</td>\n",
       "      <td>-0.413130</td>\n",
       "      <td>0.041587</td>\n",
       "      <td>0.030463</td>\n",
       "      <td>-0.414440</td>\n",
       "      <td>0.016954</td>\n",
       "      <td>-0.196070</td>\n",
       "      <td>0.445080</td>\n",
       "      <td>-0.48607</td>\n",
       "      <td>-0.345130</td>\n",
       "      <td>-0.153900</td>\n",
       "      <td>0.018211</td>\n",
       "      <td>0.056174</td>\n",
       "      <td>0.124790</td>\n",
       "      <td>-0.136790</td>\n",
       "      <td>0.031663</td>\n",
       "      <td>0.720310</td>\n",
       "      <td>-0.091717</td>\n",
       "      <td>0.11960</td>\n",
       "      <td>0.270430</td>\n",
       "      <td>-0.022466</td>\n",
       "      <td>0.55137</td>\n",
       "      <td>-0.138640</td>\n",
       "      <td>0.140370</td>\n",
       "      <td>-0.217250</td>\n",
       "      <td>-0.31669</td>\n",
       "      <td>-0.173440</td>\n",
       "      <td>-0.237840</td>\n",
       "      <td>-0.241870</td>\n",
       "      <td>-0.012594</td>\n",
       "      <td>0.23626</td>\n",
       "      <td>-0.152350</td>\n",
       "      <td>-0.21312</td>\n",
       "      <td>-0.184580</td>\n",
       "      <td>0.024633</td>\n",
       "      <td>0.039906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>-0.619840</td>\n",
       "      <td>-0.31242</td>\n",
       "      <td>0.399180</td>\n",
       "      <td>0.48442</td>\n",
       "      <td>0.174360</td>\n",
       "      <td>0.424940</td>\n",
       "      <td>-3.1734</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>-0.455390</td>\n",
       "      <td>-0.65336</td>\n",
       "      <td>0.170440</td>\n",
       "      <td>-0.11730</td>\n",
       "      <td>0.55500</td>\n",
       "      <td>0.213810</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>0.177990</td>\n",
       "      <td>-0.412120</td>\n",
       "      <td>0.339260</td>\n",
       "      <td>-0.305810</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>0.050631</td>\n",
       "      <td>0.17718</td>\n",
       "      <td>-0.253260</td>\n",
       "      <td>-0.148710</td>\n",
       "      <td>0.38141</td>\n",
       "      <td>0.029286</td>\n",
       "      <td>-0.021158</td>\n",
       "      <td>-0.579240</td>\n",
       "      <td>0.375810</td>\n",
       "      <td>-0.642010</td>\n",
       "      <td>0.078625</td>\n",
       "      <td>0.240960</td>\n",
       "      <td>0.864620</td>\n",
       "      <td>-0.309520</td>\n",
       "      <td>-0.165320</td>\n",
       "      <td>-0.207650</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.059644</td>\n",
       "      <td>-0.32430</td>\n",
       "      <td>0.309630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27342</td>\n",
       "      <td>0.239320</td>\n",
       "      <td>-0.123590</td>\n",
       "      <td>0.251170</td>\n",
       "      <td>-0.267460</td>\n",
       "      <td>-0.009448</td>\n",
       "      <td>-0.161960</td>\n",
       "      <td>0.271720</td>\n",
       "      <td>-0.317610</td>\n",
       "      <td>0.574420</td>\n",
       "      <td>-0.168850</td>\n",
       "      <td>0.306540</td>\n",
       "      <td>0.22519</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>-0.739160</td>\n",
       "      <td>-0.121870</td>\n",
       "      <td>0.633160</td>\n",
       "      <td>0.082624</td>\n",
       "      <td>1.259500</td>\n",
       "      <td>0.097352</td>\n",
       "      <td>-0.069293</td>\n",
       "      <td>0.091564</td>\n",
       "      <td>-0.77943</td>\n",
       "      <td>-0.073698</td>\n",
       "      <td>-0.128180</td>\n",
       "      <td>0.12361</td>\n",
       "      <td>0.035258</td>\n",
       "      <td>-0.494880</td>\n",
       "      <td>0.905790</td>\n",
       "      <td>0.47882</td>\n",
       "      <td>-0.318380</td>\n",
       "      <td>-0.287080</td>\n",
       "      <td>0.174230</td>\n",
       "      <td>0.069032</td>\n",
       "      <td>-0.54077</td>\n",
       "      <td>-0.180070</td>\n",
       "      <td>-0.42689</td>\n",
       "      <td>0.534840</td>\n",
       "      <td>0.213970</td>\n",
       "      <td>0.202850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1         2    ...       297       298       299\n",
       "like  -0.014949  0.17910 -0.288830  ...  0.059178 -0.192210  0.456200\n",
       "good  -0.069254  0.37668 -0.169580  ... -0.220280  0.088971 -0.028352\n",
       "well   0.088205  0.40155 -0.044703  ...  0.046184  0.079135  0.095058\n",
       "great -0.033819  0.27263 -0.203010  ... -0.184580  0.024633  0.039906\n",
       "free  -0.619840 -0.31242  0.399180  ...  0.534840  0.213970  0.202850\n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('POSITIVE WORDS')\n",
    "pos_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "AFcWVpf0NZ-V",
    "outputId": "8e9d5b63-9731-4074-ece3-905357d37c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE WORDS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>-0.380650</td>\n",
       "      <td>0.332450</td>\n",
       "      <td>0.053904</td>\n",
       "      <td>0.199670</td>\n",
       "      <td>-0.031819</td>\n",
       "      <td>-0.48597</td>\n",
       "      <td>-3.8727</td>\n",
       "      <td>0.14551</td>\n",
       "      <td>-0.307920</td>\n",
       "      <td>-0.73145</td>\n",
       "      <td>0.620760</td>\n",
       "      <td>-0.25283</td>\n",
       "      <td>0.19368</td>\n",
       "      <td>-0.612000</td>\n",
       "      <td>0.182140</td>\n",
       "      <td>0.50387</td>\n",
       "      <td>-0.080498</td>\n",
       "      <td>-0.523260</td>\n",
       "      <td>-0.607350</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>-0.11780</td>\n",
       "      <td>0.065659</td>\n",
       "      <td>0.140310</td>\n",
       "      <td>-0.12898</td>\n",
       "      <td>-0.26162</td>\n",
       "      <td>-0.091026</td>\n",
       "      <td>-0.041549</td>\n",
       "      <td>-0.18930</td>\n",
       "      <td>0.116160</td>\n",
       "      <td>0.189510</td>\n",
       "      <td>0.14205</td>\n",
       "      <td>-0.002358</td>\n",
       "      <td>-0.12047</td>\n",
       "      <td>-0.510650</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.063296</td>\n",
       "      <td>0.05119</td>\n",
       "      <td>-0.716130</td>\n",
       "      <td>0.194310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042888</td>\n",
       "      <td>-0.19716</td>\n",
       "      <td>-0.679420</td>\n",
       "      <td>-0.58452</td>\n",
       "      <td>0.13436</td>\n",
       "      <td>-0.065681</td>\n",
       "      <td>-0.286690</td>\n",
       "      <td>-0.313940</td>\n",
       "      <td>0.076297</td>\n",
       "      <td>0.455970</td>\n",
       "      <td>0.156790</td>\n",
       "      <td>0.26614</td>\n",
       "      <td>-0.102740</td>\n",
       "      <td>-0.023730</td>\n",
       "      <td>0.327520</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.20824</td>\n",
       "      <td>0.21859</td>\n",
       "      <td>0.084789</td>\n",
       "      <td>-0.082523</td>\n",
       "      <td>0.82005</td>\n",
       "      <td>0.250770</td>\n",
       "      <td>0.281840</td>\n",
       "      <td>0.390730</td>\n",
       "      <td>-0.28449</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.41290</td>\n",
       "      <td>0.251210</td>\n",
       "      <td>-0.452410</td>\n",
       "      <td>0.24153</td>\n",
       "      <td>0.027313</td>\n",
       "      <td>-0.20323</td>\n",
       "      <td>-0.264750</td>\n",
       "      <td>0.072299</td>\n",
       "      <td>0.373390</td>\n",
       "      <td>0.278730</td>\n",
       "      <td>0.086626</td>\n",
       "      <td>0.33142</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.008644</td>\n",
       "      <td>-0.186530</td>\n",
       "      <td>-0.507370</td>\n",
       "      <td>0.082950</td>\n",
       "      <td>-0.318540</td>\n",
       "      <td>0.14439</td>\n",
       "      <td>-3.7237</td>\n",
       "      <td>0.24818</td>\n",
       "      <td>0.282390</td>\n",
       "      <td>-0.69299</td>\n",
       "      <td>0.547180</td>\n",
       "      <td>0.21330</td>\n",
       "      <td>-0.12298</td>\n",
       "      <td>-0.367120</td>\n",
       "      <td>-0.067024</td>\n",
       "      <td>0.47459</td>\n",
       "      <td>0.064185</td>\n",
       "      <td>0.019239</td>\n",
       "      <td>-0.519690</td>\n",
       "      <td>-0.063528</td>\n",
       "      <td>0.11405</td>\n",
       "      <td>0.059072</td>\n",
       "      <td>0.362800</td>\n",
       "      <td>0.21275</td>\n",
       "      <td>-0.28035</td>\n",
       "      <td>-0.101910</td>\n",
       "      <td>-0.319050</td>\n",
       "      <td>-0.25206</td>\n",
       "      <td>0.235390</td>\n",
       "      <td>0.406310</td>\n",
       "      <td>-0.11798</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.19078</td>\n",
       "      <td>-0.362710</td>\n",
       "      <td>-0.047735</td>\n",
       "      <td>0.331360</td>\n",
       "      <td>-0.027431</td>\n",
       "      <td>-0.15579</td>\n",
       "      <td>-0.047158</td>\n",
       "      <td>0.142730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117520</td>\n",
       "      <td>0.17990</td>\n",
       "      <td>0.035626</td>\n",
       "      <td>-0.14481</td>\n",
       "      <td>0.29794</td>\n",
       "      <td>0.176450</td>\n",
       "      <td>0.192720</td>\n",
       "      <td>-0.515530</td>\n",
       "      <td>0.311280</td>\n",
       "      <td>0.197870</td>\n",
       "      <td>-0.357860</td>\n",
       "      <td>0.28173</td>\n",
       "      <td>-0.148110</td>\n",
       "      <td>0.085546</td>\n",
       "      <td>-0.407660</td>\n",
       "      <td>-0.48031</td>\n",
       "      <td>-0.26337</td>\n",
       "      <td>0.09894</td>\n",
       "      <td>-0.015299</td>\n",
       "      <td>-0.173200</td>\n",
       "      <td>1.14990</td>\n",
       "      <td>-0.376010</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>0.605850</td>\n",
       "      <td>0.10763</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.257580</td>\n",
       "      <td>0.32265</td>\n",
       "      <td>-0.120860</td>\n",
       "      <td>-0.231390</td>\n",
       "      <td>0.20938</td>\n",
       "      <td>-0.112420</td>\n",
       "      <td>0.34707</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.169220</td>\n",
       "      <td>-0.345110</td>\n",
       "      <td>-0.080478</td>\n",
       "      <td>-0.031539</td>\n",
       "      <td>0.13018</td>\n",
       "      <td>-0.123230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>-0.063281</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.028091</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.133950</td>\n",
       "      <td>-0.19249</td>\n",
       "      <td>-4.0381</td>\n",
       "      <td>0.84850</td>\n",
       "      <td>0.115130</td>\n",
       "      <td>-0.43636</td>\n",
       "      <td>0.042788</td>\n",
       "      <td>0.11502</td>\n",
       "      <td>0.02805</td>\n",
       "      <td>-0.104590</td>\n",
       "      <td>0.524450</td>\n",
       "      <td>0.13222</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>-0.131270</td>\n",
       "      <td>-0.044898</td>\n",
       "      <td>0.116240</td>\n",
       "      <td>0.18102</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>-0.43207</td>\n",
       "      <td>-0.48241</td>\n",
       "      <td>-0.318990</td>\n",
       "      <td>-0.030649</td>\n",
       "      <td>-0.36972</td>\n",
       "      <td>-0.038913</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>-0.49461</td>\n",
       "      <td>-0.284530</td>\n",
       "      <td>0.41345</td>\n",
       "      <td>-0.079134</td>\n",
       "      <td>-0.500330</td>\n",
       "      <td>0.107490</td>\n",
       "      <td>0.192290</td>\n",
       "      <td>0.14994</td>\n",
       "      <td>0.191390</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263280</td>\n",
       "      <td>-0.31308</td>\n",
       "      <td>0.039342</td>\n",
       "      <td>-0.33084</td>\n",
       "      <td>-0.26902</td>\n",
       "      <td>-0.054458</td>\n",
       "      <td>0.068823</td>\n",
       "      <td>0.273380</td>\n",
       "      <td>-0.094346</td>\n",
       "      <td>0.409770</td>\n",
       "      <td>0.441430</td>\n",
       "      <td>-0.14692</td>\n",
       "      <td>0.209080</td>\n",
       "      <td>-0.204990</td>\n",
       "      <td>-0.131870</td>\n",
       "      <td>-0.12501</td>\n",
       "      <td>-0.18298</td>\n",
       "      <td>0.57121</td>\n",
       "      <td>0.268840</td>\n",
       "      <td>0.027864</td>\n",
       "      <td>1.03570</td>\n",
       "      <td>-0.081028</td>\n",
       "      <td>-0.021926</td>\n",
       "      <td>0.041709</td>\n",
       "      <td>-0.26873</td>\n",
       "      <td>0.168690</td>\n",
       "      <td>0.388830</td>\n",
       "      <td>0.15005</td>\n",
       "      <td>0.137060</td>\n",
       "      <td>-0.272600</td>\n",
       "      <td>0.21323</td>\n",
       "      <td>0.025240</td>\n",
       "      <td>0.25836</td>\n",
       "      <td>-0.232000</td>\n",
       "      <td>-0.070793</td>\n",
       "      <td>-0.089673</td>\n",
       "      <td>-0.282660</td>\n",
       "      <td>-0.022416</td>\n",
       "      <td>0.62997</td>\n",
       "      <td>-0.230460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue</th>\n",
       "      <td>-0.104890</td>\n",
       "      <td>-0.165060</td>\n",
       "      <td>0.246150</td>\n",
       "      <td>-0.023061</td>\n",
       "      <td>0.393660</td>\n",
       "      <td>0.14549</td>\n",
       "      <td>-3.0342</td>\n",
       "      <td>1.01570</td>\n",
       "      <td>-0.278750</td>\n",
       "      <td>0.13990</td>\n",
       "      <td>0.168310</td>\n",
       "      <td>0.16860</td>\n",
       "      <td>-0.14396</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>0.456880</td>\n",
       "      <td>0.21366</td>\n",
       "      <td>0.560030</td>\n",
       "      <td>0.366790</td>\n",
       "      <td>0.097037</td>\n",
       "      <td>-0.042809</td>\n",
       "      <td>0.18336</td>\n",
       "      <td>0.255340</td>\n",
       "      <td>0.170960</td>\n",
       "      <td>-0.22161</td>\n",
       "      <td>-0.75806</td>\n",
       "      <td>-0.034919</td>\n",
       "      <td>-0.304990</td>\n",
       "      <td>0.13687</td>\n",
       "      <td>-0.284560</td>\n",
       "      <td>0.458650</td>\n",
       "      <td>-0.41854</td>\n",
       "      <td>-0.361040</td>\n",
       "      <td>0.49855</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>-0.209730</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>0.058053</td>\n",
       "      <td>-0.33831</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>-0.018842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.450680</td>\n",
       "      <td>-0.30460</td>\n",
       "      <td>-0.017377</td>\n",
       "      <td>0.27277</td>\n",
       "      <td>-0.16158</td>\n",
       "      <td>0.051448</td>\n",
       "      <td>0.120450</td>\n",
       "      <td>0.042021</td>\n",
       "      <td>0.386470</td>\n",
       "      <td>0.083351</td>\n",
       "      <td>0.303260</td>\n",
       "      <td>0.11033</td>\n",
       "      <td>0.049430</td>\n",
       "      <td>-0.123620</td>\n",
       "      <td>0.107120</td>\n",
       "      <td>-0.20613</td>\n",
       "      <td>-0.14211</td>\n",
       "      <td>0.26996</td>\n",
       "      <td>0.137870</td>\n",
       "      <td>-0.341250</td>\n",
       "      <td>0.14029</td>\n",
       "      <td>-0.069014</td>\n",
       "      <td>0.104610</td>\n",
       "      <td>-0.178650</td>\n",
       "      <td>-0.79129</td>\n",
       "      <td>-0.114620</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>-0.36264</td>\n",
       "      <td>0.049899</td>\n",
       "      <td>-0.529340</td>\n",
       "      <td>-0.33198</td>\n",
       "      <td>0.324220</td>\n",
       "      <td>0.37596</td>\n",
       "      <td>0.216560</td>\n",
       "      <td>0.265880</td>\n",
       "      <td>-0.160840</td>\n",
       "      <td>0.265780</td>\n",
       "      <td>-0.012321</td>\n",
       "      <td>0.39380</td>\n",
       "      <td>-0.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problems</th>\n",
       "      <td>-0.149890</td>\n",
       "      <td>-0.003204</td>\n",
       "      <td>0.279860</td>\n",
       "      <td>0.020525</td>\n",
       "      <td>-0.011720</td>\n",
       "      <td>-0.21362</td>\n",
       "      <td>-4.1626</td>\n",
       "      <td>0.51624</td>\n",
       "      <td>-0.022924</td>\n",
       "      <td>-0.15809</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.13318</td>\n",
       "      <td>-0.10316</td>\n",
       "      <td>-0.040136</td>\n",
       "      <td>0.261390</td>\n",
       "      <td>0.15421</td>\n",
       "      <td>0.232580</td>\n",
       "      <td>-0.386430</td>\n",
       "      <td>-0.208550</td>\n",
       "      <td>0.132910</td>\n",
       "      <td>-0.24978</td>\n",
       "      <td>0.346970</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>-0.40150</td>\n",
       "      <td>-0.42437</td>\n",
       "      <td>-0.278320</td>\n",
       "      <td>0.418440</td>\n",
       "      <td>-0.46474</td>\n",
       "      <td>-0.365150</td>\n",
       "      <td>0.163720</td>\n",
       "      <td>-0.55178</td>\n",
       "      <td>-0.297170</td>\n",
       "      <td>0.35740</td>\n",
       "      <td>0.061921</td>\n",
       "      <td>-0.334220</td>\n",
       "      <td>-0.057856</td>\n",
       "      <td>0.085993</td>\n",
       "      <td>-0.25830</td>\n",
       "      <td>-0.053560</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182850</td>\n",
       "      <td>-0.26764</td>\n",
       "      <td>-0.096463</td>\n",
       "      <td>-0.43541</td>\n",
       "      <td>-0.37842</td>\n",
       "      <td>-0.172080</td>\n",
       "      <td>0.191380</td>\n",
       "      <td>0.073905</td>\n",
       "      <td>-0.035921</td>\n",
       "      <td>0.455510</td>\n",
       "      <td>0.034762</td>\n",
       "      <td>0.10922</td>\n",
       "      <td>0.052386</td>\n",
       "      <td>-0.005646</td>\n",
       "      <td>-0.063221</td>\n",
       "      <td>-0.23697</td>\n",
       "      <td>-0.23857</td>\n",
       "      <td>0.68197</td>\n",
       "      <td>0.343040</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.90607</td>\n",
       "      <td>0.027487</td>\n",
       "      <td>-0.239720</td>\n",
       "      <td>0.123160</td>\n",
       "      <td>-0.50890</td>\n",
       "      <td>-0.034900</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.24597</td>\n",
       "      <td>0.327060</td>\n",
       "      <td>0.026998</td>\n",
       "      <td>0.40821</td>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0.28232</td>\n",
       "      <td>-0.318000</td>\n",
       "      <td>-0.127560</td>\n",
       "      <td>-0.344250</td>\n",
       "      <td>-0.246030</td>\n",
       "      <td>0.077485</td>\n",
       "      <td>0.46232</td>\n",
       "      <td>-0.014215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2    ...       297      298       299\n",
       "hard     -0.380650  0.332450  0.053904  ...  0.086626  0.33142  0.201400\n",
       "bad       0.008644 -0.186530 -0.507370  ... -0.031539  0.13018 -0.123230\n",
       "problem  -0.063281  0.028477  0.028091  ... -0.022416  0.62997 -0.230460\n",
       "issue    -0.104890 -0.165060  0.246150  ... -0.012321  0.39380 -0.343750\n",
       "problems -0.149890 -0.003204  0.279860  ...  0.077485  0.46232 -0.014215\n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('NEGATIVE WORDS')\n",
    "neg_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nBZlXTql5zO"
   },
   "source": [
    "Now we make arrays of the desired inputs and outputs. The inputs are the embeddings, and the outputs are `1` for positive words and `-1` for negative words. We also make sure to keep track of the words they’re labeled with, so we can interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:05:03.485323Z",
     "start_time": "2019-03-29T15:05:03.466320Z"
    },
    "id": "nfZCWDrhl5zP"
   },
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:05:05.256478Z",
     "start_time": "2019-03-29T15:05:05.210348Z"
    },
    "id": "iIfARgJwl5zR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oom499g5LnBq"
   },
   "source": [
    "As a model we use a linear classifier to associate a positive or negative value to every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTmyzbtv4eQi",
    "outputId": "c1017a75-00e6-4268-8f13-711d5b190ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9366515837104072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SGDClassifier(loss='log', random_state=0, max_iter=100,penalty='l2',learning_rate='optimal')\n",
    "model.fit(train_vectors, train_targets)\n",
    "print('Accuracy score:', accuracy_score(model.predict(test_vectors), test_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Jr1XW0l5zd"
   },
   "source": [
    "We see that the model correctly classifies 93.6% of the words. \n",
    "\n",
    "Now we define a function to assign a value to each of the words. In this way we can compare how positive or negative each word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:06:07.705034Z",
     "start_time": "2019-03-29T15:06:07.670604Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "uto0hQHsl5ze",
    "outputId": "0120919e-8f8c-4ba0-a4e9-50f443eeb33c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>10.941293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>8.097425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>1.114217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enough</th>\n",
       "      <td>3.779289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>6.400027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>16.503665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super</th>\n",
       "      <td>6.441012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>18.230530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>11.897525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dead</th>\n",
       "      <td>-6.002061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miss</th>\n",
       "      <td>1.620820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage</th>\n",
       "      <td>-11.542994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easier</th>\n",
       "      <td>3.052455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awards</th>\n",
       "      <td>12.843730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crazy</th>\n",
       "      <td>-4.581415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doubt</th>\n",
       "      <td>-0.404257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comfort</th>\n",
       "      <td>12.653283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dedicated</th>\n",
       "      <td>14.429945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stress</th>\n",
       "      <td>-4.859441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bright</th>\n",
       "      <td>9.426805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentiment\n",
       "best       10.941293\n",
       "support     8.097425\n",
       "hard        1.114217\n",
       "enough      3.779289\n",
       "important   6.400027\n",
       "amazing    16.503665\n",
       "super       6.441012\n",
       "wonderful  18.230530\n",
       "awesome    11.897525\n",
       "dead       -6.002061\n",
       "miss        1.620820\n",
       "damage    -11.542994\n",
       "easier      3.052455\n",
       "awards     12.843730\n",
       "crazy      -4.581415\n",
       "doubt      -0.404257\n",
       "comfort    12.653283\n",
       "dedicated  14.429945\n",
       "stress     -4.859441\n",
       "bright      9.426805"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vecs_to_sentiment(vecs):    \n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[embeddings.index.intersection(words)]\n",
    "    #vecs = embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# Show 20 examples from the test set\n",
    "words_to_sentiment(test_labels).iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLhV9zcuO4Bh"
   },
   "source": [
    "We see that the word \"best\" has a high positive value and the word \"damage\" has a high negative value. This means that the model believes that \"best\" is much more positive than \"damage\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4jD8Z-Ml5zj"
   },
   "source": [
    "## Sentiment score for text\n",
    "\n",
    "Now we want to calculate the sentiment of a text, instead of a word. There are many ways to combine sentiments for word vectors into an overall sentiment score. The simplest way is to average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:07:29.746290Z",
     "start_time": "2019-03-29T15:07:29.740768Z"
    },
    "id": "BSmQUGHyl5zk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:07:40.366537Z",
     "start_time": "2019-03-29T15:07:40.351478Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zF0UYAI7l5zm",
    "outputId": "5b065ac1-b647-4b3c-acfe-304764810aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.357973128784749"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment(\"this example is pretty cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:07:48.548136Z",
     "start_time": "2019-03-29T15:07:48.540902Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5PNvRukl5zp",
    "outputId": "a078c30c-619d-4bf5-a1ad-9649aa5bd436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.281248232996652"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment(\"meh, this example sucks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LKP9Menl5zs"
   },
   "source": [
    "Let’s see what it does with a few variations on a neutral sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:08:39.114595Z",
     "start_time": "2019-03-29T15:08:39.099228Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pOBv_gvl5zt",
    "outputId": "1388d03c-60a7-4291-c820-95df60cedf6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6915326468393819\n",
      "0.9642028400776796\n",
      "-0.23989800215068335\n"
     ]
    }
   ],
   "source": [
    "print(text_to_sentiment(\"Let's go get Italian food\"))\n",
    "print(text_to_sentiment(\"Let's go get Chinese food\"))\n",
    "print(text_to_sentiment(\"Let's go get Mexican food\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:09:13.624994Z",
     "start_time": "2019-03-29T15:09:13.604583Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wk1Dr88Ll5zw",
    "outputId": "c6728d18-cbb4-43d7-86a8-fde5e06553d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.798746405376553\n",
      "2.2161741111136797\n",
      "0.8275610081478063\n",
      "-0.37412977185289664\n"
     ]
    }
   ],
   "source": [
    "print(text_to_sentiment(\"My name is Emily\"))\n",
    "print(text_to_sentiment(\"My name is Heather\"))\n",
    "print(text_to_sentiment(\"My name is Ebony\"))\n",
    "print(text_to_sentiment(\"My name is Shaniqua\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxXcIHPdl5zz"
   },
   "source": [
    "The system has associated wildly different sentiments with people’s names. You can look at these examples and many others and see that the sentiment is generally more positive for stereotypically-white names, and more negative for stereotypically-black names. Therefore we conclude that our model **is racist**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmqM28wvl5z0"
   },
   "source": [
    "## Ethical problem\n",
    "\n",
    "The previous example seemed to suggest that our model was racist, since it associate different sentiments to names from different cultures. Let's inspect further this issue..\n",
    "\n",
    "Here we have four lists of names that tend to reflect different ethnic backgrounds, mostly from a United States perspective. The first two are lists of predominantly “white” and “black” names. I also added typically Hispanic names, as well as Muslim names that come from Arabic or Urdu; these are two more distinct groupings of given names that tend to represent your background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:10:18.816997Z",
     "start_time": "2019-03-29T15:10:18.787085Z"
    },
    "id": "pFML8hx3l5z2"
   },
   "outputs": [],
   "source": [
    "NAMES_BY_ETHNICITY = {\n",
    "    # The first two lists are from the Caliskan et al. appendix describing the\n",
    "    # Word Embedding Association Test.\n",
    "    'White': [\n",
    "        'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
    "        'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
    "        'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
    "        'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
    "        'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
    "        'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily',\n",
    "        'Megan', 'Rachel', 'Wendy'\n",
    "    ],\n",
    "\n",
    "    'Black': [\n",
    "        'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome',\n",
    "        'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun',\n",
    "        'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol',\n",
    "        'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle',\n",
    "        'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha',\n",
    "        'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya',\n",
    "        'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn',\n",
    "        'Tawanda', 'Yvette'\n",
    "    ],\n",
    "    \n",
    "    # This list comes from statistics about common Hispanic-origin names in the US.\n",
    "    'Hispanic': [\n",
    "        'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián',\n",
    "        'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás',\n",
    "        'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina',\n",
    "        'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
    "    ],\n",
    "    \n",
    "    # The following list conflates religion and ethnicity, I'm aware. So do given names.\n",
    "    #\n",
    "    # This list was cobbled together from searching baby-name sites for common Muslim names,\n",
    "    # as spelled in English. I did not ultimately distinguish whether the origin of the name\n",
    "    # is Arabic or Urdu or another language.\n",
    "    #\n",
    "    # I'd be happy to replace it with something more authoritative, given a source.\n",
    "    'Arab/Muslim': [\n",
    "        'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
    "        'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam',\n",
    "        'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
    "        'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi1sTMkMl5z5"
   },
   "source": [
    "Now we’ll use Pandas to make a table of these names, their predominant ethnic background, and the sentiment score we get for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:10:46.744680Z",
     "start_time": "2019-03-29T15:10:46.676412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "0QNQIP55l5z6",
    "outputId": "a5fd2853-b4a6-4f7d-e19c-ee346e67ba79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>2.445307</td>\n",
       "      <td>Arab/Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alya</th>\n",
       "      <td>5.154039</td>\n",
       "      <td>Arab/Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latoya</th>\n",
       "      <td>1.166186</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diego</th>\n",
       "      <td>3.366440</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nicolás</th>\n",
       "      <td>-0.675397</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rachel</th>\n",
       "      <td>2.236061</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wilbur</th>\n",
       "      <td>-2.462063</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment        group\n",
       "ali       2.445307  Arab/Muslim\n",
       "alya      5.154039  Arab/Muslim\n",
       "latoya    1.166186        Black\n",
       "diego     3.366440     Hispanic\n",
       "nicolás  -0.675397     Hispanic\n",
       "rachel    2.236061        White\n",
       "wilbur   -2.462063        White"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def name_sentiment_table():\n",
    "    frames = []\n",
    "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
    "        lower_names = [name.lower() for name in name_list]\n",
    "        sentiments = words_to_sentiment(lower_names)\n",
    "        sentiments['group'] = group\n",
    "        frames.append(sentiments)\n",
    "\n",
    "    # Put together the data we got from each ethnic group into one big table\n",
    "    return pd.concat(frames)\n",
    "\n",
    "name_sentiments = name_sentiment_table()\n",
    "\n",
    "name_sentiments.iloc[::25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T15:11:09.382514Z",
     "start_time": "2019-03-29T15:11:09.222382Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "iEwEVxk_l50E",
    "outputId": "fe9de1d3-52bd-48bd-8161-b1876920b5af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWW0lEQVR4nO3de5gldX3n8feHblAMGENm1jYKjrquN2Jw7aDCriJqVleja0QNWR01MfMk+7jeVkeNBomb3XXHrPcYnaBRFHXxjnfRBzGCBGa4CnjbKAmtrTMSkJtKN9/9o6qHM03PzJmZPl3dXe/X88zTp35V59S3a06fT9Wvqn4nVYUkqX8O6LoASVI3DABJ6ikDQJJ6ygCQpJ4yACSpp8a7LmBvrFmzptatW9d1GZK0omzdunV7Va2d376iAmDdunVs2bKl6zIkaUVJctVC7XYBSVJPdR4AScaSXJTkM13XIkl90nkAAC8Cruy6CEnqm04DIMk9gCcCp3RZhyT1UddHAG8GNgK37mqBJBuSbEmyZdu2bUtXmSStcp0FQJInAT+pqq27W66qNlfVZFVNrl17u6uYJEn7qMsjgGOBJyf5AfBh4PgkH+iwHknqlc7uA6iqVwGvAkhyHPCyqnpWV/VI0u5s3LiR6enpHdPbt29nZmaG8fFx1qxZs6N9YmKCTZs2dVHiXltRN4JJUlemp6eZmpq6Xfvs7OyC7SvBsgiAqvoq8NWOy5CkXZqYmNhpenp6mtnZWcbGxnaaN3+55WxZBIAkLXfzu3XWr1/P1NQUExMTnHrqqR1VtX+6vgxUktQRA0CSesoAkKSeMgAkqacMAEnqKQNAknrKAJCknjIAJKmnDABJ6ikDQJJ6ygCQpJ4yACSppwwASeopA0CSesoAkKSeMgAkqacMAEnqKQNAknrKAJCknjIAJKmn/FJ4SSvO2Y98VNclcPP4GCTcfPXVndbzqK+dvc/P9QhAknrKAJCknjIAJKmnOguAJHdMcn6SS5JcnuQvuqpFkvqoy5PAvwCOr6obkhwIfD3J56vqvA5rkqTe6CwAqqqAG9rJA9t/1VU9ktQ3nZ4DSDKW5GLgJ8CZVfUPXdYjSX3SaQBU1WxVHQXcAzg6yZHzl0myIcmWJFu2bdu29EVK0iq1LK4CqqprgbOAxy8wb3NVTVbV5Nq1a5e+OElapbq8Cmhtkru0jw8GHgd8q6t6JKlvurwK6G7A+5KM0QTR6VX1mQ7rkaRe6fIqoEuBh3S1fknqu2VxDkCStPQMAEnqKQNAknrKAJCknjIAJKmnDABJ6ikDQJJ6ygCQpJ4yACSppwwASeqpLscCkqQV4wNjB3BtsmP62oGfbx8f29F+lyqeNXvr0ha3jwwASRrCtQnXDATAnFsTrumgnsVgAEjSEO5SO39j7fXALDAGHLqb5ZYzA0CShrBSunX2hieBJamnDABJ6ikDQJJ6ygCQpJ4yACSpp7wKSFqlNm7cyPT09I7p7du3MzMzw/j4OGvWrNnRPjExwaZNm7ooUR0zAKRVanp6mqmpqdu1z87OLtiu/jEApFVqYmJip+np6WlmZ2cZGxvbad785dQfBoC0Ss3v1lm/fj1TU1NMTExw6qmndlSVlhNPAktSTxkAktRTBoAk9VRnAZDk8CRnJbkiyeVJXtRVLZLUR12eBJ4B/ltVXZjkUGBrkjOr6ooOa5Kk3ujsCKCqflRVF7aPrweuBO7eVT2S1DfL4hxAknXAQ4B/WGDehiRbkmzZtm3bUpcmSatW5wGQ5BDgY8CLq+pn8+dX1eaqmqyqybVr1y59gZK0SnUaAEkOpPnwP62qPt5lLZLUN52dBE4S4N3AlVX1xq7q2F8OuCVpperyKqBjgWcDlyW5uG37s6r6XIc17TUH3JK0UnUWAFX1dSBdrX+xOOCWpJXKweD2kwNuSVqpOr8KSJLUDQNAknrKAJCknjIAJKmnhgqAJMcO0yZJWjmGPQJ425BtkqQVYreXgSZ5BHAMsDbJSwdm3RkYG2VhkqTR2tN9AAcBh7TLHTrQ/jPghFEVJUkavd0GQFWdDZyd5L1VddUS1SRJWgLD3gl8hySbgXWDz6mq40dRlCRp9IYNgI8A7wROAWZHV44kaakMGwAzVfU3I61EkrSkhr0M9NNJ/kuSuyU5bO7fSCuTJI3UsEcAz2l/vnygrYB7L245kqSlMlQAVNW9Rl2IJGlpDTsUxJ2SvKa9Eogk903ypNGWJkkapWHPAfwd8Euau4IBpoC/HElFkqQlMew5gPtU1TOTnAhQVTe1X+ouaUjHvq3b8RMPuvYgDuAA/vnaf+68lnP+6zmdrl+NYY8AfpnkYJoTvyS5D/CLkVUlSRq5YY8AXgt8ATg8yWnAscBzR1WUJGn0hr0K6MwkFwIPBwK8qKq2j7QySdJI7c03gt2dZgjog4BHJvm90ZQkSVoKQx0BJHkP8GDgcuDWtrmAj4+oLknSiA17DuDhVfXAkVYiSVpSw3YBfSOJASBJq8iwRwCn0oTANM3lnwGqqh68Pytvu5aeBPykqo7cn9eSJO2dYQPg3cCzgcu47RzAYngv8HaagJEkLaFhA2BbVZ2x2Cuvqq8lWbfYrytJ2rNhA+CiJB8EPs3AHcBVNfKrgJJsADYAHHHEEaNenST1xrABcDDNB//vDLQtyWWgVbUZ2AwwOTlZo16fJPXFsHcCP2/UhSyGh768+1MJh26/njHgn7Zf32k9W9+wvrN1S1oZdhsASTZW1aYkb6MdCG5QVb1wZJVJkkZqT0cAV7Y/t4xi5Uk+BBwHrElyNfDaqnr3KNYlSdrZbgOgqj7dPrypqj4yOC/J0/d35VV14v6+hiRp3wx7J/CrhmyTJK0QezoH8ATgPwJ3T/LWgVl3BmZGWZgkabT2dA7ghzT9/08Gtg60Xw+8ZFRFSZJGb0/nAC4BLknywaq6ZYlqkiQtgWFvBDs6ycnAPdvnzA0Gd+9RFSZJGq29GQzuJTTdQLOjK0eStFSGDYDrqurzI61EkrSkhg2As5K8gWbsn8HB4C4cSVWSpJEbNgAe1v6cHGgr4PjFLUeStFSGHQzu0aMuRJK0tIa6EzjJXZO8O8nn2+kHJvmj0ZYmSRqlYYeCeC/wReA32unvAC8eRUGSpKUxbACsqarTab8PuKpm8HJQSVrRhg2AG5P8Ou13AiR5OHDdyKqSJI3csFcBvRQ4A7hPknOAtcAJI6tKkjRywwbAfYAnAIcDT6O5LHTY50rqwIHnHEhuyo7puce5KRx05kE72utOxS3HOtRXHw3bBfTnVfUz4NeARwPvAP5mZFVJ2m+5KRxw4wE7/qXaAKh57QMhoX4ZNgDmTvg+EfjbqvoscNBulpfUsbpTceuv3LrjX40VlaLG5rXf6XZf962eGLYbZyrJu4DHAf87yR0YPjwkdcBuHe3JsB/iz6C5D+A/VNW1wGHAy0dWlSRp5IYdCuImmoHg5qZ/BPxoVEVJkkbPbhxJ6ikDQJJ6ygCQpJ4yACSppzq9mzfJ44G3AGPAKVX1+i7rUbc2btzI9PT0Tm3bt29nZmaG8fFx1qxZs6N9YmKCTZs2LXWJ0qrSWQAkGQP+mubegquBC5KcUVVXdFWTujU9Pc3U1NSC82ZnZ3c5T9K+6fII4Gjge1X1jwBJPgw8BTAAempiYuJ2bdPT08zOzjI2NrbT/IWWlbR3UtXNbeBJTgAeX1XPb6efDTysql4wb7kNwAaAI4444qFXXXXVkte6O/O7LXb3gbXSuiz+6XW/2XUJvPL8w/jxzePc9eAZXn/0NZ3VccRJl3W2bml/JdlaVZPz25f9iJ5VtRnYDDA5ObnsBi3ZVbeFXRaSlrsuA2CKZnjpOfdo21aU+V0RuztpKUnLSZcBcAFw3yT3ovng/33gDzqsZ5+stG4dSZrTWQBU1UySF9AMMjcGvKeqLu+qHknqm07PAVTV54DPdVmDJPWVdwJLUk8ZAJLUUwaAJPWUASBJPWUASFJPGQCS1FMGgCT1lAEgST1lAEhSTxkAktRTBoAk9ZQBIEk9ZQBIUk8ZAJLUUwaAJPWUASBJPWUASFJPGQCS1FMGgCT1lAEgST1lAEhSTxkAktRTBoAk9ZQBIEk9ZQBIUk8ZAJLUU+NdrDTJ04GTgQcAR1fVli7q0O4dcdJlXZfA+Pr1MDXF+GH35IiTzu66HGlV6eoI4JvA7wFf62j9ktR7nRwBVNWVAEm6WL0kiRVwDiDJhiRbkmzZtm1b1+VI0qoxsiOAJF8GJhaY9eqq+tSwr1NVm4HNAJOTk7VI5UlS740sAKrqsaN6bUnS/lv2XUCSpNHoJACSPDXJ1cAjgM8m+WIXdUhSn3V1FdAngE90sW5JUsMuIEnqKQNAknrKAJCknjIAJKmnDABJ6ikDQJJ6ygCQpJ4yACSppwwASeopA0CSesoAkKSeMgAkqacMAEnqKQNAknrKAJCknjIAJKmnDABJ6ikDQJJ6ygCQpJ4yACSppwwASeopA0CSemq86wKkORs3bmR6enqntrnp6elp1q9fv6N9YmKCTZs2LWl90mpjAGjZmJ6eZmpqasF5s7Ozu5wnad8YAFo2JiYmbte2fft2ZmZmGB8fZ82aNbtdVtLe6SQAkrwB+F3gl8D/A55XVdd2UYuWD7t0pKXV1UngM4Ejq+rBwHeAV3VUhyT1VicBUFVfqqqZdvI84B5d1CFJfbYcLgP9Q+Dzu5qZZEOSLUm2bNu2bQnLkqTVbWTnAJJ8GVjoTN2rq+pT7TKvBmaA03b1OlW1GdgMMDk5WSMoVZJ6aWQBUFWP3d38JM8FngQ8pqr8YJekJdbVVUCPBzYCj6qqm7qoQZL6rqtzAG8HDgXOTHJxknd2VIck9VZWUu9Lkm3AVV3XMYQ1wPaui1hF3J6Lx225uFbK9rxnVa2d37iiAmClSLKlqia7rmO1cHsuHrfl4lrp23M5XAYqSeqAASBJPWUAjMbmrgtYZdyei8dtubhW9Pb0HIAk9ZRHAJLUUwaAJPXUqg+AJP8pSSW5/z4894bdzHt4kr9Nclz7+s8fmHdU2/ayfaz5hvbnbyT56L68xnKVZLa9+e+SJBcmOaZtX5fkm/v4ml9NsmIvxdud+e/BJM9N8vb28Z8kWb/wMxe9jtcl2e3wLitZkjclefHA9BeTnDIw/X+SvDTJZ3bx/FOSPLB9/Gejr3hxrPoAAE4Evt7+3EmS/RkK4wnAF9rH3wSeMW+dl+zHawNQVT+sqhP293WWmZur6qiq+i2a74H4X10XtFJV1Tur6tQlWtdJVfXlpVhXR84B5nZGDqC5wetBA/OPAQ7a1ZOr6vlVdUU7aQAsB0kOAf4d8EfA77dtxyX5+yRnAFe0bZ9MsjXJ5Uk2zHuNN7XtX0kyeCfdY4C5P4irgDsmuWuSAI9nYIjrwT3UJGuS/KB9/KAk57d7xJcmue+8de/YK273/D6Z5MwkP0jygnaP5KIk5yU5bJE221K6M/Av8xvb3/vv2yOEHUcJ7bxXJLmsPYJ4/bznHZDkvUn+cglq71ySk+eOMpO8MMkV7fvowwPz35/kG0m+m+SP2/ZD2vfzhe22fErbvi7Jle2R7eVJvpTk4Hbee5Oc0D7+7STntv8H5yc5tJstsKjOBR7RPn4QzU7d9Ul+LckdgAcAFwKHJPlokm8lOa39e9/xN96+Jw9u/6ZPa+c9a+Dv/F1Jxpb+11vYav9O4KcAX6iq7yT5aZKHtu3/luYbyb7fTv9hVV3TvtkvSPKxqvop8CvAlqp6SZKTgNcCL0iyBrilqq5r//8BPgo8HbiI5o3yiyHq+xPgLVV1WpKDgD29MY4EHgLcEfge8IqqekiSNwHrgTcPsc6uHZzkYprf4W7A8Qss8xPgcVX18zYUPwRMJnkCzf/pw6rqpnmhN04zrPg3q+p/jPZXWFJz22vOYcAZCyz3SuBeVfWLJHcZaH8w8HCa9/JFST5Ls32fWlU/a9/L57U7RAD3BU6sqj9OcjrwNOADcy/Wvk//L/DMqrogyZ2BmxfnV+1OVf0wyUySI2j29r8B3J0mFK4DLqP5CtuH0ATED2mOGo6l6WGYe51XJnlBVR0FkOQBwDOBY6vqliTvAP4zsCRHbnuy2gPgROAt7eMPt9OfAc4f+PAHeGGSp7aPD6f5I/gpcCvNmx2aP4KPt49/B/jSvHWd3i57f5oPrGPYs28Ar05yD+DjVfXdPSx/VlVdT7Nnch3w6bb9Mpo/9JXg5oE/jkcApyY5ct4yBwJvT3IUMAv8m7b9scDfzY0gW1XXDDznXcDpq+zDHwa2F+wYRn2h8x2XAqcl+STwyYH2T1XVzcDNSc4CjgY+C/zPJI+keY/fHbhru/z3q2oucLYC6+at537Aj6rqAoCq+tl+/G7Lzbk0f7fHAG+k2S7H0ATAOe0y51fV1QBtMK9jIAAW8BjgoTQ7lgAH0wTwsrBqu4DavcPjgVPaLpeX0/TTB7hxYLnjaD5YHtH2S19Es3e6kLmbJgb7/5sZVdPALcDjgK/Me94Mt23rOw4854PAk2n2oD6XZKG94UGDRxW3DkzfygoM86r6Bk1f6/xBql4C/Bj4LZoPu132vQ44F3h0kl393612TwT+mubo9oLcdn5r/o0+RbMHuhZ4aBsuP+a29+Xge2yWFfi+2g9z5wF+k6YL6DyaI4BjaN5fsPfbJ8D72vNeR1XV/arq5EWtej+s2gAATgDeX1X3rKp1VXU48H3g389b7leBf2m7FO5Pc7g854D2dQD+APh62+f3YOBibu8kmm6Z2XntP6DZC5irC4Ak9wb+sareCnyKlbMXvyja7T1Gc7Q16Fdp9jJvBZ7NbV1jZwLPS3Kn9vmDXUDvBj4HnJ79O7m/4qQ5aXl4VZ0FvIJm+x3Szn5Kkjsm+XXgOOCCdv5P2i6JRwP33IvVfRu4W5Lfbtd96Cra3ufSfEnVNVU12x5h3oUmBM7d7TN3dkuSA9vHXwFOSPKvoHnPJtmb7T1SqzkATgQ+Ma/tY9z+aqAvAONJrgReT5P6c24Ejk5zIvZ44HU0H+QXLfQtZlV1blV9cn478FfAnya5iGaPd84zgG+2h5JHskz6BUds7gTZxTRdZs9ZIDDfATwnySU0XWo3AlTVF2j6v7e0z9/pMtuqeiPNEdz72w/FvhgDPpDkMprf/61VdW0771LgLJr39X+vqh/SnCuZbJdfD3xr2BVV1S9p+rTf1v7/nMmuj5hXmsto/j7Pm9d2XVXtzZDPm4FLk5zWXhn0GuBLSS6l2V53W6yC95dDQeylJK8BvldVH+66Fml3kpwM3FBVf9V1LVqeVsuh25Kpql5cYihp9fMIQJJ6qk/9pJKkAQaAJPWUASBJPWUASFJPGQDSXlpFNz6p5wwAaZ4kf57k20m+nuRDSV7Wjvb45iRbgBcleUyakVgvS/KedsRI0ozUuqZ9PJnkq+3jBUfmlLrknow0oB3i4Gk04xAdSDOy69Z29kFVNdmON/Rd4DHtSLOnAn/Knkdjvd3InO2duVInPAKQdnYszQiaP29HXv30wLy5kWHvRzNq5nfa6fcBjxzitT9VVTe3wwrMjcwpdcYAkIZ3454XWXjk19ZCI3NKnTEApJ2dA/xuO4LmITSjQ873bWBdkn/dTj8bOLt9/ANuG/n1afOet9DInFJnDABpQPtFJ2fQjKL5edrRIOct83PgecBH2hE1bwXe2c7+C+At7cni+aOcLjQyp9QZxwKS5klySFXd0H7vwNeADVV14X6+5sk4MqeWGa8Ckm5vc5IH0vThv29/P/yl5cojAEnqKc8BSFJPGQCS1FMGgCT1lAEgST1lAEhST/1/qk1JARUk7awAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUgISjOcinwB"
   },
   "source": [
    "In these previous plots we can see that the classifier is racist, especially for black names. Moreover, the most positive-valued names are white names. This happens because there is a racist bias in the data used to train the classifier. Therefore, the more accurate our classifier is with the data, the more it learns its undesired biases. \n",
    "\n",
    "The key idea to take from this is that **the model is racist because the data we used to train it is racist**. After all, the model only learns from data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLo4HeKOQ_-V"
   },
   "source": [
    "## Discussion topics\n",
    "\n",
    "1. Why is the model racist?\n",
    "\n",
    "2. How could you try to solve this issue?\n",
    "\n",
    "3. If a racist model gives better accuracy results than a non-racist model, which one would you use?\n",
    "\n",
    "4. What other situations can you think of where biases among data can make a machine learning model discriminate among people? \n",
    "\n",
    "5. What other ethical problems could there emerge when working with data (i.e data privacy, anonimization,...)?\n",
    "\n",
    "6. If you want to discover some of your ethica values you can play this game:  https://www.moralmachine.net/hl/es\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de Assignment 2. Ethics and Sentiment Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
